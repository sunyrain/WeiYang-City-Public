{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化网络：设计网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Initialize a MLP2 Neural Network === \n",
      "self.layer1 -- weight: torch.Size([64, 128])  bias: torch.Size([64])\n",
      "self.layer2 -- weight: torch.Size([8, 64])  bias: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP2,self).__init__()\n",
    "\n",
    "        print(' === Initialize a MLP2 Neural Network === ')    \n",
    "\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        print('self.layer1 -- weight:',self.layer1.weight.shape,' bias:',self.layer1.bias.shape )\n",
    "        print('self.layer2 -- weight:',self.layer2.weight.shape,' bias:',self.layer2.bias.shape )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(' === MLP2 Neural Network Forward Calculation === ')    \n",
    "        x = torch.tensor(x,dtype=torch.float32)\n",
    "        print('origin input -- x:',x.shape)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        print('after layer 1 -- x1:',x1.shape)\n",
    "\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x2 = self.layer2(x1)\n",
    "        print('after layer 2 -- x2:',x2.shape)\n",
    "\n",
    "        return x2\n",
    "\n",
    "# hyper parameters\n",
    "input_dim = 128 # e.g. data input with 128 features\n",
    "hidden_dim = 64 # you can define your own net structure\n",
    "output_dim = 8 # e.g. a classification task of 8 class\n",
    "class_count = 8\n",
    "\n",
    "# initialize our net\n",
    "MLP_Net = MLP2(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化数据, 进行前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Input Tensor Initialization === \n",
      "input_tensor: torch.Size([5, 128])\n",
      " === Real Label Initialization === \n",
      "real_label: tensor([0, 5, 6, 7, 2])\n",
      "real_one_hot_label: tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# initialize input \n",
    "batch_size = 5\n",
    "\n",
    "print(' === Input Tensor Initialization === ')    \n",
    "input_tensor = torch.ones([batch_size, input_dim])\n",
    "print('input_tensor:',input_tensor.shape)\n",
    "\n",
    "\n",
    "print(' === Real Label Initialization === ')    \n",
    "real_label = torch.randint(low=0,high=8,size= [batch_size] )#3 # 假设该数据实际分类结果应该为第3类\n",
    "print('real_label:',real_label)\n",
    "\n",
    "real_one_hot_label  = torch.eye(class_count)[real_label,:]\n",
    "print('real_one_hot_label:',real_one_hot_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Get Output === \n",
      " === MLP2 Neural Network Forward Calculation === \n",
      "origin input -- x: torch.Size([5, 128])\n",
      "after layer 1 -- x1: torch.Size([5, 64])\n",
      "after layer 2 -- x2: torch.Size([5, 8])\n",
      "output_tensor: torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "# get output\n",
    "print(' === Get Output === ')    \n",
    "output_tensor = MLP_Net(input_tensor)\n",
    "print('output_tensor:',output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算损失函数（target）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(2.1131, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output_tensor, real_one_hot_label)\n",
    "print('loss = ',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算梯度并更新 Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward Calculation\n",
      "-->name: layer1.weight -->grad_requirs: True  -->grad_value: None\n",
      "-->name: layer1.bias -->grad_requirs: True  -->grad_value: None\n",
      "-->name: layer2.weight -->grad_requirs: True  -->grad_value: None\n",
      "-->name: layer2.bias -->grad_requirs: True  -->grad_value: None\n",
      "After backward Calculation\n",
      "-->name: layer1.weight -->grad_requirs: True  -->grad_value: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0348, 0.0348, 0.0348,  ..., 0.0348, 0.0348, 0.0348],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "-->name: layer1.bias -->grad_requirs: True  -->grad_value: tensor([ 0.0000,  0.0348,  0.0000,  0.0153, -0.0435,  0.0058,  0.0000, -0.0079,\n",
      "         0.0180,  0.0129,  0.0000, -0.0071,  0.0000,  0.0000,  0.0079, -0.0220,\n",
      "         0.0118,  0.0164,  0.0088, -0.0278,  0.0000,  0.0000,  0.0147,  0.0000,\n",
      "         0.0000,  0.0002,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0199,\n",
      "        -0.0049,  0.0000,  0.0000,  0.0105,  0.0111,  0.0000,  0.0152, -0.0054,\n",
      "        -0.0232,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0037,  0.0101,\n",
      "         0.0034,  0.0000,  0.0000,  0.0219, -0.0085,  0.0051,  0.0000,  0.0000,\n",
      "         0.0193,  0.0000,  0.0000,  0.0047,  0.0216,  0.0000,  0.0000,  0.0000])\n",
      "-->name: layer2.weight -->grad_requirs: True  -->grad_value: tensor([[ 0.0000e+00, -2.5259e-04,  0.0000e+00, -3.3293e-02, -1.4093e-03,\n",
      "         -1.0044e-01,  0.0000e+00, -2.5728e-02, -1.4954e-02, -3.8816e-03,\n",
      "          0.0000e+00, -1.9269e-02,  0.0000e+00,  0.0000e+00, -1.6206e-02,\n",
      "         -1.6297e-02, -3.1200e-02, -1.9684e-02, -1.9031e-02, -1.1474e-02,\n",
      "          0.0000e+00,  0.0000e+00, -1.8467e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -2.9146e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -3.6857e-02, -8.5200e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -4.4376e-04, -2.9837e-02,  0.0000e+00, -2.7142e-02, -1.2120e-02,\n",
      "         -3.6274e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -1.1558e-02, -1.7296e-02, -1.1112e-02,  0.0000e+00,\n",
      "          0.0000e+00, -3.1464e-02, -1.8747e-02, -4.9027e-04,  0.0000e+00,\n",
      "          0.0000e+00, -3.8190e-02,  0.0000e+00,  0.0000e+00, -2.7330e-02,\n",
      "         -5.7125e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  4.9943e-04,  0.0000e+00,  6.5828e-02,  2.7865e-03,\n",
      "          1.9859e-01,  0.0000e+00,  5.0870e-02,  2.9567e-02,  7.6749e-03,\n",
      "          0.0000e+00,  3.8099e-02,  0.0000e+00,  0.0000e+00,  3.2042e-02,\n",
      "          3.2223e-02,  6.1690e-02,  3.8919e-02,  3.7628e-02,  2.2687e-02,\n",
      "          0.0000e+00,  0.0000e+00,  3.6514e-02,  0.0000e+00,  0.0000e+00,\n",
      "          5.7628e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  7.2874e-02,  1.6846e-02,  0.0000e+00,  0.0000e+00,\n",
      "          8.7741e-04,  5.8994e-02,  0.0000e+00,  5.3665e-02,  2.3965e-02,\n",
      "          7.1722e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  2.2853e-02,  3.4197e-02,  2.1970e-02,  0.0000e+00,\n",
      "          0.0000e+00,  6.2211e-02,  3.7067e-02,  9.6937e-04,  0.0000e+00,\n",
      "          0.0000e+00,  7.5510e-02,  0.0000e+00,  0.0000e+00,  5.4037e-02,\n",
      "          1.1295e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -3.9448e-04,  0.0000e+00, -5.1995e-02, -2.2009e-03,\n",
      "         -1.5686e-01,  0.0000e+00, -4.0180e-02, -2.3354e-02, -6.0621e-03,\n",
      "          0.0000e+00, -3.0093e-02,  0.0000e+00,  0.0000e+00, -2.5309e-02,\n",
      "         -2.5452e-02, -4.8726e-02, -3.0741e-02, -2.9721e-02, -1.7920e-02,\n",
      "          0.0000e+00,  0.0000e+00, -2.8841e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -4.5518e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -5.7561e-02, -1.3306e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -6.9304e-04, -4.6597e-02,  0.0000e+00, -4.2388e-02, -1.8929e-02,\n",
      "         -5.6651e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -1.8051e-02, -2.7011e-02, -1.7354e-02,  0.0000e+00,\n",
      "          0.0000e+00, -4.9138e-02, -2.9278e-02, -7.6567e-04,  0.0000e+00,\n",
      "          0.0000e+00, -5.9642e-02,  0.0000e+00,  0.0000e+00, -4.2682e-02,\n",
      "         -8.9214e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  5.9231e-04,  0.0000e+00,  7.8071e-02,  3.3047e-03,\n",
      "          2.3552e-01,  0.0000e+00,  6.0331e-02,  3.5066e-02,  9.1023e-03,\n",
      "          0.0000e+00,  4.5185e-02,  0.0000e+00,  0.0000e+00,  3.8002e-02,\n",
      "          3.8216e-02,  7.3163e-02,  4.6158e-02,  4.4626e-02,  2.6907e-02,\n",
      "          0.0000e+00,  0.0000e+00,  4.3305e-02,  0.0000e+00,  0.0000e+00,\n",
      "          6.8346e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  8.6428e-02,  1.9979e-02,  0.0000e+00,  0.0000e+00,\n",
      "          1.0406e-03,  6.9966e-02,  0.0000e+00,  6.3646e-02,  2.8422e-02,\n",
      "          8.5062e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  2.7104e-02,  4.0558e-02,  2.6057e-02,  0.0000e+00,\n",
      "          0.0000e+00,  7.3782e-02,  4.3962e-02,  1.1497e-03,  0.0000e+00,\n",
      "          0.0000e+00,  8.9554e-02,  0.0000e+00,  0.0000e+00,  6.4087e-02,\n",
      "          1.3396e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  5.2601e-04,  0.0000e+00,  6.9332e-02,  2.9348e-03,\n",
      "          2.0916e-01,  0.0000e+00,  5.3578e-02,  3.1141e-02,  8.0834e-03,\n",
      "          0.0000e+00,  4.0127e-02,  0.0000e+00,  0.0000e+00,  3.3748e-02,\n",
      "          3.3938e-02,  6.4973e-02,  4.0991e-02,  3.9631e-02,  2.3895e-02,\n",
      "          0.0000e+00,  0.0000e+00,  3.8457e-02,  0.0000e+00,  0.0000e+00,\n",
      "          6.0695e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  7.6753e-02,  1.7743e-02,  0.0000e+00,  0.0000e+00,\n",
      "          9.2412e-04,  6.2134e-02,  0.0000e+00,  5.6522e-02,  2.5240e-02,\n",
      "          7.5540e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  2.4070e-02,  3.6018e-02,  2.3140e-02,  0.0000e+00,\n",
      "          0.0000e+00,  6.5522e-02,  3.9041e-02,  1.0210e-03,  0.0000e+00,\n",
      "          0.0000e+00,  7.9529e-02,  0.0000e+00,  0.0000e+00,  5.6913e-02,\n",
      "          1.1896e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -4.3945e-04,  0.0000e+00, -5.7922e-02, -2.4518e-03,\n",
      "         -1.7474e-01,  0.0000e+00, -4.4761e-02, -2.6016e-02, -6.7532e-03,\n",
      "          0.0000e+00, -3.3523e-02,  0.0000e+00,  0.0000e+00, -2.8194e-02,\n",
      "         -2.8353e-02, -5.4281e-02, -3.4245e-02, -3.3109e-02, -1.9963e-02,\n",
      "          0.0000e+00,  0.0000e+00, -3.2128e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -5.0707e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -6.4122e-02, -1.4823e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -7.7204e-04, -5.1909e-02,  0.0000e+00, -4.7220e-02, -2.1087e-02,\n",
      "         -6.3109e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -2.0109e-02, -3.0091e-02, -1.9332e-02,  0.0000e+00,\n",
      "          0.0000e+00, -5.4740e-02, -3.2616e-02, -8.5295e-04,  0.0000e+00,\n",
      "          0.0000e+00, -6.6441e-02,  0.0000e+00,  0.0000e+00, -4.7547e-02,\n",
      "         -9.9384e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -3.0083e-04,  0.0000e+00, -3.9651e-02, -1.6784e-03,\n",
      "         -1.1962e-01,  0.0000e+00, -3.0641e-02, -1.7809e-02, -4.6229e-03,\n",
      "          0.0000e+00, -2.2949e-02,  0.0000e+00,  0.0000e+00, -1.9301e-02,\n",
      "         -1.9409e-02, -3.7158e-02, -2.3443e-02, -2.2665e-02, -1.3666e-02,\n",
      "          0.0000e+00,  0.0000e+00, -2.1994e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -3.4712e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -4.3895e-02, -1.0147e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -5.2850e-04, -3.5535e-02,  0.0000e+00, -3.2325e-02, -1.4435e-02,\n",
      "         -4.3202e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -1.3766e-02, -2.0599e-02, -1.3234e-02,  0.0000e+00,\n",
      "          0.0000e+00, -3.7472e-02, -2.2327e-02, -5.8389e-04,  0.0000e+00,\n",
      "          0.0000e+00, -4.5483e-02,  0.0000e+00,  0.0000e+00, -3.2549e-02,\n",
      "         -6.8034e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -2.3041e-04,  0.0000e+00, -3.0369e-02, -1.2855e-03,\n",
      "         -9.1617e-02,  0.0000e+00, -2.3469e-02, -1.3641e-02, -3.5408e-03,\n",
      "          0.0000e+00, -1.7577e-02,  0.0000e+00,  0.0000e+00, -1.4783e-02,\n",
      "         -1.4866e-02, -2.8460e-02, -1.7955e-02, -1.7359e-02, -1.0467e-02,\n",
      "          0.0000e+00,  0.0000e+00, -1.6845e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -2.6586e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -3.3620e-02, -7.7718e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -4.0479e-04, -2.7217e-02,  0.0000e+00, -2.4758e-02, -1.1056e-02,\n",
      "         -3.3089e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -1.0543e-02, -1.5777e-02, -1.0136e-02,  0.0000e+00,\n",
      "          0.0000e+00, -2.8701e-02, -1.7101e-02, -4.4721e-04,  0.0000e+00,\n",
      "          0.0000e+00, -3.4836e-02,  0.0000e+00,  0.0000e+00, -2.4930e-02,\n",
      "         -5.2108e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "-->name: layer2.bias -->grad_requirs: True  -->grad_value: tensor([-0.0605,  0.1197, -0.0945,  0.1420,  0.1261, -0.1053, -0.0721, -0.0552])\n"
     ]
    }
   ],
   "source": [
    "# 定义optimizer\n",
    "learning_rate = 0.001 # 可自定义学习率，调参\n",
    "optimizer = optim.Adam(MLP_Net.parameters(), lr=learning_rate)\n",
    "\n",
    "# 清空梯度，准备计算新的梯度\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# backward 利用pytorch自动求导\n",
    "print('Before backward Calculation')\n",
    "for name, parms in MLP_Net.named_parameters():\t\n",
    "    print('-->name:', name, '-->grad_requirs:',parms.requires_grad, \\\n",
    "        ' -->grad_value:',parms.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward Calculation')\n",
    "for name, parms in MLP_Net.named_parameters():\t\n",
    "    print('-->name:', name, '-->grad_requirs:',parms.requires_grad, \\\n",
    "        ' -->grad_value:',parms.grad)\n",
    "    \n",
    "# 进行一步更新\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === MLP2 Neural Network Forward Calculation === \n",
      "origin input -- x: torch.Size([5, 128])\n",
      "after layer 1 -- x1: torch.Size([5, 64])\n",
      "after layer 2 -- x2: torch.Size([5, 8])\n",
      "new loss =  tensor(2.0584, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# 查看更新后的loss  \n",
    "new_output_tensor = MLP_Net(input_tensor)\n",
    "loss = loss_fn(new_output_tensor, real_one_hot_label)\n",
    "print('new loss = ',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
